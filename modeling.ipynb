{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Ubiquant data\n",
    "\n",
    "This notebook reproduces this [Kaggle submission](https://www.kaggle.com/code/valleyzw/ubiquant-lgbm-baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from argparse import Namespace\n",
    "from collections import defaultdict\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, GroupKFold, train_test_split, KFold\n",
    "\n",
    "from utils import reduce_mem_usage\n",
    "from lgbm_ops import rmse, feval_rmse, feval_pearsonr, weighted_average, run\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "YELLOW = '#FFBC00'\n",
    "GREEN = '#37795D'\n",
    "PURPLE = '#5460C0'\n",
    "BACKGROUND = '#F4EBE6'\n",
    "colors = [GREEN, PURPLE]\n",
    "custom_params = {\n",
    "    'axes.spines.right': False, 'axes.spines.top': False,\n",
    "    'axes.facecolor':BACKGROUND, 'figure.facecolor': BACKGROUND, \n",
    "    'figure.figsize':(8, 8)\n",
    "}\n",
    "sns_palette = sns.color_palette(colors, len(colors))\n",
    "sns.set_theme(style='ticks', rc=custom_params)\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    debug=False,\n",
    "    seed=21,\n",
    "    folds=5,\n",
    "    workers=4,\n",
    "    min_time_id=None, \n",
    "    holdout=False,\n",
    "    cv_method=\"group\",\n",
    "    num_bins=16,\n",
    "    holdout_size=100,\n",
    "    outlier_threshold=0.001,\n",
    "    trading_days_per_year=250,   # chinese stock market trading days per year (roughly)\n",
    "    add_investment_id_model=False,\n",
    "    data_path=Path(\".\"),\n",
    "    just_eda=False,\n",
    ")\n",
    "seed_everything(args.seed)\n",
    "\n",
    "if args.debug:\n",
    "    setattr(args, 'min_time_id', 1150)\n",
    "\n",
    "assert args.cv_method in {\"kfold\", \"group\", \"stratified\", \"time\", \"group_time\", \"time_range\"}, \"unknown cv method\"\n",
    "assert args.data_path.exists(), \"data_path not exists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(args.data_path.joinpath(\"train_low_mem.parquet\"))\n",
    "assert train.isnull().any().sum() == 0, \"null exists.\"\n",
    "assert train.row_id.str.extract(r\"(?P<time_id>\\d+)_(?P<investment_id>\\d+)\").astype(train.time_id.dtype).equals(train[[\"time_id\", \"investment_id\"]]), \"row_id!=time_id_investment_id\"\n",
    "assert train.time_id.is_monotonic_increasing, \"time_id not monotonic increasing\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock market calendar analysis: [discussion](https://www.kaggle.com/c/ubiquant-market-prediction/discussion/309720)\n",
    "> The Chinese stock market turbulence began with the popping of the stock market bubble on 12 June 2015 and ended in early February 2016. - [wikipedia](https://en.wikipedia.org/wiki/2015%E2%80%932016_Chinese_stock_market_turbulence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df = pd.read_csv(\"./holidays_of_china_from_2014_to_2030.csv\", parse_dates=[\"date\"], date_parser=lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "display(calendar_df.head())\n",
    "\n",
    "# leave only national holidays\n",
    "calendar_df = calendar_df.loc[(calendar_df.type.isin([\"National holiday\", \"Common local holiday\"]))]\n",
    "display(calendar_df.head())\n",
    "\n",
    "# fill with everyday from 2014 to 2022\n",
    "calendar_df = (\n",
    "    pd.DataFrame({\"date\": pd.date_range(start=\"2014-01-01\", end=\"2022-01-01\")}).merge(calendar_df, on=\"date\", how=\"left\")\n",
    "    .assign(weekday=lambda x: x.date.dt.day_name(), year=lambda x: x.date.dt.year)\n",
    ")\n",
    "display(calendar_df.head())\n",
    "\n",
    "# remove weekends and national holidays and align with time_id\n",
    "calendar_df = (\n",
    "    calendar_df.loc[(~calendar_df.weekday.isin([\"Sunday\", \"Saturday\"]))&(calendar_df.name.isna())]\n",
    "    .reset_index(drop=True)\n",
    "    .head(train.time_id.max()+1)\n",
    "    .dropna(axis=1)\n",
    ")\n",
    "display(calendar_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_time_id_range = range(train.time_id.min(), train.time_id.max()+1)\n",
    "f, (ax0, ax1, ax2) = plt.subplots(3, 1, gridspec_kw={'height_ratios': [1,1,6]}, figsize=(10,10), sharex=True, dpi=128)\n",
    "_df = (\n",
    "    train[['time_id', 'investment_id']]\n",
    "    .groupby(\"time_id\")\n",
    "    .count()\n",
    "    .reindex(full_time_id_range)\n",
    "    .set_index(calendar_df.date)\n",
    ")\n",
    "peeks, _ = find_peaks(-_df.values.squeeze(), threshold=200)\n",
    "_df.plot(ax=ax0, color=PURPLE)\n",
    "ax0.set_xticks(ticks=_df.iloc[peeks].index.values, minor=True)\n",
    "ax0.set_ylabel(\"count\")\n",
    "ax0.legend(loc='upper left')\n",
    "\n",
    "(\n",
    "    train[['time_id', 'target']]\n",
    "    .groupby(\"time_id\")\n",
    "    .mean()\n",
    "    .reindex(full_time_id_range)\n",
    "    .set_index(calendar_df.date)\n",
    "    .plot(ax=ax1, color=PURPLE)\n",
    ")\n",
    "ax1.axvspan(*mdates.date2num(_df.loc[(_df.index>\"2015-06\")&(_df.index<\"2016-03\")].index[[0,-1]]), fill=True, alpha=0.9, color=YELLOW)\n",
    "ax1.set_ylabel(\"mean\")\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "_df = (\n",
    "    train[['investment_id', 'time_id', \"target\"]]\n",
    "    .pivot_table(index=\"time_id\", columns=\"investment_id\", values=\"target\", aggfunc=\"count\")\n",
    "    .reindex(full_time_id_range)\n",
    "    .set_index(calendar_df.date)\n",
    ")\n",
    "ax2.imshow(_df.T, cmap='winter', interpolation='nearest', aspect=\"auto\", origin=\"lower\", alpha=0.6, extent=[*mdates.date2num([calendar_df.date.min(), calendar_df.date.max()]), train.investment_id.min(), train.investment_id.max()])\n",
    "ax2.set_xlabel(\"date\")\n",
    "ax2.set_ylabel(\"investment_id\")\n",
    "ax2.xaxis_date()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train[[\"investment_id\", \"target\"]].groupby(\"investment_id\").target.mean()\n",
    "upper_bound, lower_bound = df.quantile([1-args.outlier_threshold, args.outlier_threshold])\n",
    "ax = df.plot(figsize=(16, 8), color=PURPLE)\n",
    "ax.axhspan(lower_bound, upper_bound, fill=False, linestyle=\"--\", color=\"k\")\n",
    "plt.show()\n",
    "\n",
    "outlier_investments = df.loc[(df>upper_bound)|(df<lower_bound)|(df==0)].index\n",
    "_=pd.pivot(\n",
    "    train.loc[train.investment_id.isin(outlier_investments), [\"investment_id\", \"time_id\", \"target\"]],\n",
    "    index='time_id', columns='investment_id', values='target'\n",
    ").plot(figsize=(16,12), subplots=True, sharex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Remove feature outliers: [notebook](https://www.kaggle.com/junjitakeshima/ubiquant-simple-lgbm-removing-outliers-en-jp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_list = []\n",
    "outlier_col = []\n",
    "\n",
    "for col in (f\"f_{i}\" for i in range(300)):\n",
    "    _mean, _std = train[col].mean(), train[col].std()\n",
    "    \n",
    "    temp_df = train.loc[(train[col] > _mean + _std * 70) | (train[col] < _mean - _std * 70)]\n",
    "    temp2_df = train.loc[(train[col] > _mean + _std * 35) | (train[col] < _mean - _std * 35)]\n",
    "    if len(temp_df) >0 : \n",
    "        outliers = temp_df.index.to_list()\n",
    "        outlier_list.extend(outliers)\n",
    "        outlier_col.append(col)\n",
    "        print(col, len(temp_df))\n",
    "    elif len(temp2_df)>0 and len(temp2_df) <6 :\n",
    "        outliers = temp2_df.index.to_list()\n",
    "        outlier_list.extend(outliers)\n",
    "        outlier_col.append(col)\n",
    "        print(col, len(temp2_df))\n",
    "\n",
    "outlier_list = list(set(outlier_list))\n",
    "train.drop(train.index[outlier_list], inplace = True)\n",
    "print(len(outlier_col), len(outlier_list), train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing time IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pd.pivot(\n",
    "    train.loc[train.investment_id.isin([1,17,19,3011,3151]), [\"investment_id\", \"time_id\", \"target\"]],\n",
    "    index='time_id', columns='investment_id', values='target'\n",
    ").plot(figsize=(16,12), subplots=True, sharex=True, color=GREEN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting Time IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_features = [\"f_74\", \"f_153\", \"f_183\", \"f_145\"]\n",
    "df=train[[\"time_id\", \"target\"]+_features].groupby(\"time_id\").mean()\n",
    "time_to_cheer_up, time_to_calm_down = df.target.idxmax(), df.target.idxmin()\n",
    "ax, *_ = df.plot(figsize=(16,12), subplots=True, sharex=True, color=GREEN, alpha=.5)\n",
    "ax.axhline(0, linestyle=\"--\", color=\"k\", linewidth=1)\n",
    "ax.scatter(time_to_cheer_up, df.loc[time_to_cheer_up, \"target\"], marker=\"^\", color=GREEN)\n",
    "ax.scatter(time_to_calm_down, df.loc[time_to_calm_down, \"target\"], marker=\"v\", color=GREEN)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, *_ = train.loc[train.time_id==time_to_cheer_up, [\"investment_id\", \"target\"]+_features].plot(x=\"investment_id\", figsize=(16,12), subplots=True, sharex=True, color=GREEN, alpha=.5)\n",
    "ax.axhline(0, linestyle=\"--\", color=\"k\")\n",
    "ax, *_ = train.loc[train.time_id==time_to_calm_down, [\"investment_id\", \"target\"]+_features].plot(x=\"investment_id\", figsize=(16,12), subplots=True, sharex=True, color=GREEN, alpha=.5)\n",
    "ax.axhline(0, linestyle=\"--\", color=\"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.min_time_id is not None:\n",
    "    train = train.query(\"time_id>=@args.min_time_id\").reset_index(drop=True)\n",
    "    gc.collect()\n",
    "    \n",
    "train=train.loc[~train.investment_id.isin(outlier_investments)].reset_index(drop=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=train[[\"time_id\", \"target\"]+_features].groupby(\"time_id\").mean().plot(figsize=(16,12), subplots=True, sharex=True, color=GREEN, alpha=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_id_df = (\n",
    "    train[[\"investment_id\", \"time_id\"]]\n",
    "    .groupby(\"investment_id\")\n",
    "    .agg([\"min\", \"max\", \"count\", np.ptp])\n",
    "    .assign(\n",
    "        time_span=lambda x: x.time_id.ptp,\n",
    "        time_count=lambda x: x.time_id[\"count\"]\n",
    "    )\n",
    "    .drop(columns=\"ptp\", level=1)\n",
    "    .reset_index()\n",
    ")\n",
    "time_id_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(time_id_df.drop(columns=\"time_id\", level=0).droplevel(level=1, axis=1), on=\"investment_id\", how='left')\n",
    "train[[\"time_span\", \"time_count\"]].hist(bins=args.num_bins, figsize=(16,12), sharex=True, layout=(2,1), color=GREEN)\n",
    "max_time_span=time_id_df.time_id[\"max\"].max()\n",
    "outlier_investments = time_id_df.loc[time_id_df.time_id[\"count\"]<32, \"investment_id\"].to_list()\n",
    "del time_id_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.holdout:\n",
    "    _target = pd.cut(train.time_span, args.num_bins, labels=False)\n",
    "    _train, _valid = train_test_split(_target, stratify=_target, random_state=args.seed)\n",
    "    print(f\"train length: {len(_train)}\", f\"holdout length: {len(_valid)}\")\n",
    "    valid = train.iloc[_valid.index].sort_values(by=[\"time_id\", \"investment_id\"]).reset_index(drop=True)\n",
    "    train = train.iloc[_train.index].sort_values(by=[\"time_id\", \"investment_id\"]).reset_index(drop=True)\n",
    "    train.time_span.hist(bins=args.num_bins, figsize=(16,8), alpha=0.8)\n",
    "    valid.time_span.hist(bins=args.num_bins, figsize=(16,8), alpha=0.8)\n",
    "    valid.drop(columns=\"time_span\").to_parquet(\"valid.parquet\")\n",
    "    del valid, _train, _valid, _target\n",
    "    gc.collect()\n",
    "assert train.time_id.is_monotonic_increasing, \"time_id not monotonic increasing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cv_method==\"stratified\":\n",
    "    train[\"fold\"] = -1\n",
    "    _target = pd.cut(train.time_span, args.num_bins, labels=False)\n",
    "    skf = StratifiedKFold(n_splits=args.folds)\n",
    "    for fold, (train_index, valid_index) in enumerate(skf.split(_target, _target)):\n",
    "        train.loc[valid_index, 'fold'] = fold\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=args.folds, ncols=1, sharex=True, figsize=(16,8), tight_layout=True)\n",
    "    for ax, (fold, df) in zip(axs, train[[\"fold\", \"time_span\"]].groupby(\"fold\")):\n",
    "        ax.hist(df.time_span, bins=args.num_bins)\n",
    "        ax.text(0, 40000, f\"fold: {fold}, count: {len(df)}\", fontsize=16)\n",
    "    plt.show()\n",
    "    del _target, train_index, valid_index\n",
    "    _=gc.collect()\n",
    "if args.just_eda:\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = []\n",
    "num_features = list(train.filter(like=\"f_\").columns)\n",
    "features = num_features + cat_features\n",
    "\n",
    "combination_features = [\"f_231-f_250\", \"f_118-f_280\", \"f_155-f_297\", \"f_25-f_237\", \"f_179-f_265\", \"f_119-f_270\", \"f_71-f_197\", \"f_21-f_65\"]\n",
    "for f in combination_features:\n",
    "    f1, f2 = f.split(\"-\")\n",
    "    train[f] = train[f1] + train[f2]\n",
    "features += combination_features\n",
    "\n",
    "to_drop = [\"f_148\", \"f_72\", \"f_49\", \"f_205\", \"f_228\", \"f_97\", \"f_262\", \"f_258\"]\n",
    "features = list(sorted(set(features).difference(set(to_drop))))\n",
    "\n",
    "train = reduce_mem_usage(train.drop(columns=\"time_span\"))\n",
    "train[[\"investment_id\", \"time_id\"]] = train[[\"investment_id\", \"time_id\"]].astype(np.uint16)\n",
    "train=train.drop(columns=[\"row_id\"]+to_drop)\n",
    "\n",
    "if args.cv_method==\"stratified\":\n",
    "    train[\"fold\"] = train[\"fold\"].astype(np.uint8)\n",
    "gc.collect()\n",
    "#features += [\"time_id\"] # https://www.kaggle.com/c/ubiquant-market-prediction/discussion/302429\n",
    "features_backup = features.copy()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cv_method==\"time_range\":\n",
    "    train[\"time_range\"] = pd.cut(train.time_id, bins=int(np.ceil(max_time_span/args.trading_days_per_year)))\n",
    "    _ = train.time_range.value_counts(sort=False).plot(kind=\"barh\", figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lgbm_ops import rmse, feval_rmse, feval_pearsonr, weighted_average, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment_ids = train.investment_id.unique()\n",
    "info = \"without_investment_id\"\n",
    "features_importance = run(info=info, args=args, train=train, features=features, cat_features=cat_features)\n",
    "df = train[[\"target\", \"preds\", \"time_id\"]].query(\"preds!=-1000\")\n",
    "score = df.groupby(\"time_id\").apply(lambda x: pearsonr(x.target, x.preds)[0]).mean()\n",
    "print(f\"lgbm {info} {args.cv_method} {args.folds} folds mean rmse: {rmse(df.target, df.preds):.4f}, mean pearsonr: {pearsonr(df.target, df.preds)[0]:.4f}, mean pearsonr by time_id: {score:.4f}\")\n",
    "\n",
    "folds_mean_importance = (\n",
    "    features_importance.groupby(\"feature\", as_index=False)\n",
    "    .importance.mean()\n",
    "    .sort_values(by=\"importance\", ascending=False)\n",
    ")\n",
    "features_importance.to_csv(f\"features_importance_{info}.csv\", index=False)\n",
    "folds_mean_importance.to_csv(f\"folds_mean_feature_importance_{info}.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.subplot(1,2,1)\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=folds_mean_importance.head(50))\n",
    "plt.title(f'Head LightGBM Features {info} (avg over {args.folds} folds)')\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=folds_mean_importance.tail(50))\n",
    "plt.title(f'Tail LightGBM Features {info} (avg over {args.folds} folds)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.add_investment_id_model:\n",
    "    cat_features = [\"investment_id\"]\n",
    "    features += cat_features\n",
    "    info = \"with_investment_id\"\n",
    "    features_importance = run(info=info, args=args, train=train, features=features, cat_features=cat_features)\n",
    "    df = train[[\"target\", \"preds\", \"time_id\"]].query(\"preds!=-1000\")\n",
    "    score = df.groupby(\"time_id\").apply(lambda x: pearsonr(x.target, x.preds)[0]).mean()\n",
    "    print(f\"lgbm {info} {args.cv_method} {args.folds} folds mean rmse: {rmse(df.target, df.preds):.4f}, mean pearsonr: {pearsonr(df.target, df.preds)[0]:.4f}, mean pearsonr by time_id: {score:.4f}\")\n",
    "\n",
    "    folds_mean_importance = (\n",
    "        features_importance.groupby(\"feature\", as_index=False)\n",
    "        .importance.mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)\n",
    "    )\n",
    "    features_importance.to_csv(f\"features_importance_{info}.csv\", index=False)\n",
    "    folds_mean_importance.to_csv(f\"folds_mean_feature_importance_{info}.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=folds_mean_importance.head(50))\n",
    "    plt.title(f'Head LightGBM Features {info} (avg over {args.folds} folds)')\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=folds_mean_importance.tail(50))\n",
    "    plt.title(f'Tail LightGBM Features {info} (avg over {args.folds} folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    del df\n",
    "\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources \n",
    "- [Ubiquant LGBM Baseline](https://www.kaggle.com/code/valleyzw/ubiquant-lgbm-baseline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaflow-structured-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
